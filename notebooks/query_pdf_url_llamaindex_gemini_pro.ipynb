{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1-LZEunuBiCb"
      },
      "source": [
        "__Query Unstructured PDF URL__\n",
        "- Create a search index from a PDF file\n",
        "- Encode the text into vectors using a Google embedding model.\n",
        "- Use the index to find relevant passages/sections based on search queries.\n",
        "\n",
        "\n",
        "[h3manth.com](https://h3manth.com)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "laR0FLLTJVxJ",
        "outputId": "e30af200-ee2d-4c0e-f597-a779799ad422"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[33mWARNING: Ignoring invalid distribution -atplotlib (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -atplotlib (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.2.1+cu121 requires torch==2.2.1, but you have torch 2.2.0 which is incompatible.\n",
            "torchtext 0.17.1 requires torch==2.2.1, but you have torch 2.2.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -atplotlib (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m\u001b[33mWARNING: Ignoring invalid distribution -atplotlib (/usr/local/lib/python3.10/dist-packages)\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "!pip install langchain unstructured[pdf] unstructured chromadb -qU\n",
        "!pip install --quiet langchain-google-genai -qU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "uy3Abzc-SLbx"
      },
      "outputs": [],
      "source": [
        "import pathlib\n",
        "import textwrap\n",
        "\n",
        "from IPython.display import display\n",
        "from IPython.display import Markdown\n",
        "\n",
        "def to_markdown(text):\n",
        "  text = text.replace('â€¢', '  *')\n",
        "  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "MfDYcwJTJXk9"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "import os\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "os.environ[\"GOOGLE_API_KEY\"] = GOOGLE_API_KEY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "aBXiTTcFJFLA"
      },
      "outputs": [],
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-pro\", convert_system_message_to_human=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "1qHb2BtqJFLB"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders import UnstructuredURLLoader\n",
        "from langchain.indexes import VectorstoreIndexCreator\n",
        "from langchain.text_splitter import CharacterTextSplitter\n",
        "from langchain.embeddings import GooglePalmEmbeddings\n",
        "from langchain.chains import RetrievalQA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "CQIWZttnJFLB",
        "outputId": "702c625e-7441-4ea5-cd25-d6aaf2da978b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
          ]
        }
      ],
      "source": [
        "\"\"\"\n",
        "urls: This creates a list with a single URL pointing to a PDF file. This will be the document that is indexed.\n",
        "loader: This creates an UnstructuredURLLoader object, passing the list of URLs to index as a parameter. This loader will download and parse the content from the URLs.\n",
        "VectorstoreIndexCreator: This class from Vectorstore is used to create a search index.\n",
        "embedding: An embedding model is specified to encode the text into vectors/embeddings that can be searched. Here it uses GooglePalmEmbeddings.\n",
        "text_splitter: This splits the text into chunks before encoding as some embedding models have text length limits. Here it splits into 3000 character chunks with no overlap.\n",
        "from_loaders: This creates and populates the index using the loader(s) passed to it.\n",
        "\n",
        "So it will parse the PDF content, split into chunks, generate embeddings with the Google model, and add it to the index.\n",
        "\"\"\"\n",
        "\n",
        "urls = ['https://www.cs.kent.ac.uk/people/staff/dat/miranda/whyfp90.pdf']\n",
        "loader = [UnstructuredURLLoader(urls=urls)]\n",
        "index = VectorstoreIndexCreator(\n",
        "        embedding=GooglePalmEmbeddings(),\n",
        "        text_splitter=CharacterTextSplitter(chunk_size=3000, chunk_overlap=0)).from_loaders(loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "DLTR-FUfJFLB"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        "RetrievalQA.from_chain_type: Creates a RetrievalQA pipeline using a predefined chain/architecture.\n",
        "llm: This is specifying the language model to use for question answering.\n",
        "chain_type=\"stuff\": This selects the \"stuff\" architecture which is retrieval augmented QA. It will retrieve documents to augment the QA model's context.\n",
        "retriever=index.vectorstore.as_retriever(): This configures the vectorstore index created earlier to be used for retrieval.\n",
        "input_key=\"question\": Specifies that the input key for queries will be \"question\" (i.e. we pass queries via a \"question\" key to the pipeline).\n",
        "\"\"\"\n",
        "\n",
        "chain = RetrievalQA.from_chain_type(llm=llm,\n",
        "                            chain_type=\"stuff\",\n",
        "                            retriever=index.vectorstore.as_retriever(),\n",
        "                            input_key=\"question\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 184
        },
        "id": "X_FL4c7zJFLB",
        "outputId": "ae658143-2a04-4f58-facd-2e149482474b"
      },
      "outputs": [
        {
          "data": {
            "text/markdown": [
              "> - Functional programming emphasizes applying functions to arguments to structure software.\n",
              "> - Higher-order functions and lazy evaluation contribute significantly to modularity.\n",
              "> - Modularization simplifies software design and reduces programming costs.\n",
              "> - Foldr replaces all occurrences of Cons in a list with f and all occurrences of Nil with a.\n",
              "> - Foldr can be used to define functions like append, length, doubleall, and summatrix.\n",
              "> - Modularity allows for the reuse of general-purpose modules, faster development, and independent testing.\n",
              "> - Functional languages provide new kinds of \"glue\" to enhance modularization.\n",
              "> - Functional programming's power lies in improved modularization, leading to smaller, simpler, and more general modules."
            ],
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "summary = chain.run('Summarize the paper in 8 bullet points in markdown format line by line')\n",
        "\n",
        "to_markdown(summary)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "query url.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
