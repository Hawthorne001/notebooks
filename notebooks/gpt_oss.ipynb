{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `gpt_oss` usage\n",
    "> This comprehensive notebook demonstrates all major components of the `gpt_oss` package\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### **Core Components**\n",
    "1. **Tokenizer**: o200k_base with Harmony extensions\n",
    "2. **Stub Generation**: Testing without full models\n",
    "3. **Browser Tool**: Web search capabilities (with Exa backend)\n",
    "4. **Python Tool**: Code execution in isolated Docker containers\n",
    "5. **Model Components**: PyTorch architecture elements\n",
    "6. **Evaluation**: Answer extraction and normalization\n",
    "7. **Harmony Integration**: Message structures and roles\n",
    "8. **API Utilities**: Reasoning effort and tool routing\n",
    "\n",
    "### **Advanced Features**\n",
    "9. **PyTorch Model Architecture**: Complete transformer implementation with RMSNorm, RoPE, attention blocks\n",
    "10. **Custom Tools & Integration**: Tool creation framework and end-to-end workflows\n",
    "\n",
    "### **Backend Support**\n",
    "- **PyTorch**: Full transformer with attention, MLP, normalization\n",
    "- **Triton**: GPU-optimized kernels (requires CUDA)\n",
    "- **vLLM**: Production serving with tensor parallelism\n",
    "- **Metal**: Apple Silicon optimization\n",
    "\n",
    "### **Dependencies**\n",
    "All required packages are automatically installed in the first cell:\n",
    "- Core GPT-OSS components via PYTHONPATH\n",
    "- PyTorch for model components\n",
    "- OpenAI Harmony for message structures\n",
    "- FastAPI, aiohttp, and other utilities\n",
    "\n",
    "### **External Requirements**\n",
    "- **EXA_API_KEY**: Required for browser tool web searches\n",
    "- **Docker**: Required for Python tool code execution\n",
    "\n",
    "The `gpt-oss` package provides a production-ready, research-oriented toolkit for language model inference, evaluation, and tool integration across multiple hardware configurations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies\n",
    "\n",
    "First, let's ensure all required packages are installed and set up the Python path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installing all required packages...\n",
      "✓ All packages installed successfully\n",
      "\n",
      "✓ Added /Users/hhegadehallimadh/labs/gpt-oss to Python path\n",
      "✓ Setup complete!\n"
     ]
    }
   ],
   "source": [
    "# Install all required packages and set up Python path\n",
    "import subprocess\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Install all packages in one command\n",
    "packages = [\n",
    "    \"torch>=2.0.0\",\n",
    "    \"openai-harmony\",\n",
    "    \"tiktoken\",\n",
    "    \"safetensors>=0.5.0\",\n",
    "    \"chz>=0.3.0\",\n",
    "    \"pydantic>=2.11.7\",\n",
    "    \"fastapi>=0.116.1\",\n",
    "    \"aiohttp>=3.12.14\",\n",
    "    \"structlog\",\n",
    "    \"requests>=2.31.0\",\n",
    "    \"termcolor\",\n",
    "    \"docker>=7.1.0\"\n",
    "]\n",
    "\n",
    "print(\"Installing all required packages...\")\n",
    "try:\n",
    "    # Install all packages in one pip command\n",
    "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\"] + packages + [\"-q\"])\n",
    "    print(\"✓ All packages installed successfully\")\n",
    "except subprocess.CalledProcessError as e:\n",
    "    print(f\"✗ Package installation failed: {e}\")\n",
    "    print(\"Attempting to install packages individually...\")\n",
    "    for package in packages:\n",
    "        try:\n",
    "            subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package, \"-q\"])\n",
    "            print(f\"  ✓ {package}\")\n",
    "        except:\n",
    "            print(f\"  ✗ {package}\")\n",
    "\n",
    "# Add the GPT-OSS directory to Python path \n",
    "# This will eventually be `pip install gpt-oss`\n",
    "gpt_oss_path = '~/labs/gpt-oss'\n",
    "if gpt_oss_path not in sys.path:\n",
    "    sys.path.insert(0, gpt_oss_path)\n",
    "\n",
    "print(f\"\\n✓ Added {gpt_oss_path} to Python path\")\n",
    "print(\"✓ Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Tokenizer Usage\n",
    "\n",
    "The tokenizer is based on o200k_base with Harmony-specific extensions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: Hello, world! This is a test.\n",
      "Tokens: [13225, 11, 2375, 0, 1328, 382, 261, 1746, 13]\n",
      "Decoded: Hello, world! This is a test.\n",
      "Token count: 9\n"
     ]
    }
   ],
   "source": [
    "from gpt_oss.tokenizer import get_tokenizer\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = get_tokenizer()\n",
    "\n",
    "# Encode text to tokens\n",
    "text = \"Hello, world! This is a test.\"\n",
    "tokens = tokenizer.encode(text)\n",
    "decoded = tokenizer.decode(tokens)\n",
    "\n",
    "print(f\"Original text: {text}\")\n",
    "print(f\"Tokens: {tokens}\")\n",
    "print(f\"Decoded: {decoded}\")\n",
    "print(f\"Token count: {len(tokens)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Stub Generation\n",
    "\n",
    "Test token generation without requiring a full model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature 0.0: Next token = 17196\n",
      "Temperature 0.5: Next token = 200008\n",
      "Temperature 1.0: Next token = 17\n"
     ]
    }
   ],
   "source": [
    "from gpt_oss.responses_api.utils import stub_infer_next_token\n",
    "\n",
    "# Example tokens for \"Hello, world\"\n",
    "input_tokens = [13225, 11, 2375]\n",
    "\n",
    "# Generate next token with different temperatures\n",
    "for temp in [0.0, 0.5, 1.0]:\n",
    "    next_token = stub_infer_next_token(input_tokens, temperature=temp)\n",
    "    print(f\"Temperature {temp}: Next token = {next_token}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Browser Tool Setup\n",
    "\n",
    "Demonstrate the browser tool for web search (without requiring API key):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Browser tool classes imported successfully\n",
      "SimpleBrowserTool class: SimpleBrowserTool\n",
      "ExaBackend class: ExaBackend\n",
      "⚠️ EXA_API_KEY not set - browser tool requires this environment variable\n",
      "To use the browser tool, set: export EXA_API_KEY='your-api-key'\n",
      "Get an API key from: https://exa.ai\n"
     ]
    }
   ],
   "source": [
    "# Import browser tool classes (works without API key for import only)\n",
    "try:\n",
    "    from gpt_oss.tools.simple_browser import SimpleBrowserTool\n",
    "    from gpt_oss.tools.simple_browser.backend import ExaBackend\n",
    "    \n",
    "    print(\"Browser tool classes imported successfully\")\n",
    "    print(\"SimpleBrowserTool class:\", SimpleBrowserTool.__name__)\n",
    "    print(\"ExaBackend class:\", ExaBackend.__name__)\n",
    "    \n",
    "    # Check if API key is available\n",
    "    import os\n",
    "    if os.environ.get(\"EXA_API_KEY\"):\n",
    "        print(\"✓ EXA_API_KEY is set - browser tool can be used\")\n",
    "        \n",
    "        # Create browser tool instance\n",
    "        backend = ExaBackend(source=\"web\")\n",
    "        browser = SimpleBrowserTool(backend=backend)\n",
    "        print(f\"Tool name: {browser.name}\")\n",
    "        print(f\"Tool instruction: {browser.instruction[:100]}...\")\n",
    "    else:\n",
    "        print(\"⚠️ EXA_API_KEY not set - browser tool requires this environment variable\")\n",
    "        print(\"To use the browser tool, set: export EXA_API_KEY='your-api-key'\")\n",
    "        print(\"Get an API key from: https://exa.ai\")\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Browser tool import failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Python Docker Tool\n",
    "\n",
    "Execute Python code in isolated Docker containers (requires Docker running):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python Docker tool classes imported successfully\n",
      "✗ Docker is not running\n",
      "To use Python Docker tool:\n",
      "1. Install Docker: https://docs.docker.com/get-docker/\n",
      "2. Start Docker Desktop or Docker daemon\n",
      "3. Pull Python image: docker pull python:3.11\n"
     ]
    }
   ],
   "source": [
    "# Python Docker Tool - Execute code in isolated containers\n",
    "try:\n",
    "    from gpt_oss.tools.python_docker.docker_tool import PythonTool, call_python_script\n",
    "    import docker\n",
    "    \n",
    "    print(\"Python Docker tool classes imported successfully\")\n",
    "    \n",
    "    # Check if Docker is running\n",
    "    try:\n",
    "        docker_client = docker.from_env()\n",
    "        docker_client.ping()\n",
    "        print(\"✓ Docker is running\")\n",
    "        \n",
    "        # Example 1: Direct script execution\n",
    "        script = \"\"\"\n",
    "import math\n",
    "import numpy as np\n",
    "\n",
    "# Mathematical calculations\n",
    "print(\"Mathematical calculations:\")\n",
    "print(f\"Pi = {math.pi}\")\n",
    "print(f\"Square root of 2 = {math.sqrt(2)}\")\n",
    "\n",
    "# NumPy operations\n",
    "arr = np.array([1, 2, 3, 4, 5])\n",
    "print(f\"\\\\nNumPy array: {arr}\")\n",
    "print(f\"Mean: {np.mean(arr)}\")\n",
    "print(f\"Standard deviation: {np.std(arr)}\")\n",
    "\n",
    "# Data structure example\n",
    "data = {'name': 'GPT-OSS', 'version': '1.0', 'features': ['tokenization', 'inference', 'tools']}\n",
    "print(f\"\\\\nData structure: {data}\")\n",
    "\"\"\"\n",
    "        \n",
    "        print(\"\\nExecuting Python script in Docker container...\")\n",
    "        try:\n",
    "            output = call_python_script(script)\n",
    "            print(\"Script output:\")\n",
    "            print(output)\n",
    "        except Exception as e:\n",
    "            print(f\"Script execution failed: {e}\")\n",
    "            print(\"Note: Requires python:3.11 Docker image\")\n",
    "        \n",
    "        # Example 2: Using PythonTool with async\n",
    "        print(\"\\n--- Using PythonTool with async ---\")\n",
    "        import asyncio\n",
    "        from openai_harmony import Message, Author, Role, TextContent\n",
    "        \n",
    "        async def test_python_tool():\n",
    "            tool = PythonTool()\n",
    "            \n",
    "            # Create a message with Python code\n",
    "            code_message = Message(\n",
    "                author=Author(role=Role.USER, name=\"user\"),\n",
    "                content=[TextContent(text=\"\"\"\n",
    "# Generate Fibonacci sequence\n",
    "def fibonacci(n):\n",
    "    if n <= 0:\n",
    "        return []\n",
    "    elif n == 1:\n",
    "        return [0]\n",
    "    elif n == 2:\n",
    "        return [0, 1]\n",
    "    else:\n",
    "        fib = [0, 1]\n",
    "        for i in range(2, n):\n",
    "            fib.append(fib[-1] + fib[-2])\n",
    "        return fib\n",
    "\n",
    "result = fibonacci(10)\n",
    "print(f\"First 10 Fibonacci numbers: {result}\")\n",
    "print(f\"Sum: {sum(result)}\")\n",
    "\"\"\")]\n",
    "            ).with_recipient(\"python\")\n",
    "            \n",
    "            # Process the code\n",
    "            async for response in tool.process(code_message):\n",
    "                print(\"Tool output:\")\n",
    "                print(response.content[0].text)\n",
    "        \n",
    "        # Run async function\n",
    "        await test_python_tool()\n",
    "        \n",
    "    except docker.errors.DockerException:\n",
    "        print(\"✗ Docker is not running\")\n",
    "        print(\"To use Python Docker tool:\")\n",
    "        print(\"1. Install Docker: https://docs.docker.com/get-docker/\")\n",
    "        print(\"2. Start Docker Desktop or Docker daemon\")\n",
    "        print(\"3. Pull Python image: docker pull python:3.11\")\n",
    "        \n",
    "except ImportError as e:\n",
    "    print(f\"Python Docker tool import failed: {e}\")\n",
    "    print(\"The tool requires the 'docker' package: pip install docker\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Model Architecture Components\n",
    "\n",
    "Explore the PyTorch model components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape: torch.Size([2, 4])\n",
      "SwiGLU output shape: torch.Size([2, 2])\n",
      "Input: tensor([[ 0.4551,  0.0592,  0.0381, -1.7763],\n",
      "        [-0.6815, -0.9640,  1.2013,  1.2982]])\n",
      "Output: tensor([[ 0.3300, -0.0153],\n",
      "        [-0.0059,  2.4445]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from gpt_oss.torch.model import swiglu, sdpa\n",
    "\n",
    "# Test SwiGLU activation\n",
    "x = torch.randn(2, 4)\n",
    "output = swiglu(x)\n",
    "print(f\"Input shape: {x.shape}\")\n",
    "print(f\"SwiGLU output shape: {output.shape}\")\n",
    "print(f\"Input: {x}\")\n",
    "print(f\"Output: {output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Evaluation Framework\n",
    "\n",
    "Explore the evaluation utilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original text: The answer is \\boxed{42}.\n",
      "Extracted answer: 42\n",
      "Normalized: 42\n"
     ]
    }
   ],
   "source": [
    "from gpt_oss.evals.aime_eval import extract_boxed_text, normalize_number\n",
    "\n",
    "# Test answer extraction\n",
    "test_text = r\"The answer is \\boxed{42}.\"\n",
    "extracted = extract_boxed_text(test_text)\n",
    "normalized = normalize_number(extracted)\n",
    "\n",
    "print(f\"Original text: {test_text}\")\n",
    "print(f\"Extracted answer: {extracted}\")\n",
    "print(f\"Normalized: {normalized}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Harmony Integration\n",
    "\n",
    "Work with Harmony message structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: Hello, how are you?\n",
      "Assistant: I'm doing well, thank you for asking!\n"
     ]
    }
   ],
   "source": [
    "from openai_harmony import Message, TextContent, Author, Role\n",
    "\n",
    "# Create a user message\n",
    "user_message = Message(\n",
    "    author=Author(role=Role.USER, name=\"user\"),\n",
    "    content=[TextContent(text=\"Hello, how are you?\")]\n",
    ")\n",
    "\n",
    "# Create an assistant message\n",
    "assistant_message = Message(\n",
    "    author=Author(role=Role.ASSISTANT, name=\"assistant\"),\n",
    "    content=[TextContent(text=\"I'm doing well, thank you for asking!\")]\n",
    ")\n",
    "\n",
    "print(f\"User: {user_message.content[0].text}\")\n",
    "print(f\"Assistant: {assistant_message.content[0].text}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. API Server Components\n",
    "\n",
    "Explore API server utilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Effort 'low' maps to: ReasoningEffort.LOW\n",
      "Effort 'medium' maps to: ReasoningEffort.MEDIUM\n",
      "Effort 'high' maps to: ReasoningEffort.HIGH\n",
      "Tool 'browser' is custom: True\n",
      "Tool 'python' is custom: False\n",
      "Tool 'builtin_tool' is custom: True\n"
     ]
    }
   ],
   "source": [
    "from gpt_oss.responses_api.api_server import get_reasoning_effort, is_not_builtin_tool\n",
    "\n",
    "# Test reasoning effort mapping\n",
    "for effort in [\"low\", \"medium\", \"high\"]:\n",
    "    reasoning_effort = get_reasoning_effort(effort)\n",
    "    print(f\"Effort '{effort}' maps to: {reasoning_effort}\")\n",
    "\n",
    "# Test tool routing\n",
    "tools = [\"browser\", \"python\", \"builtin_tool\"]\n",
    "for tool in tools:\n",
    "    is_custom = is_not_builtin_tool(tool)\n",
    "    print(f\"Tool '{tool}' is custom: {is_custom}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. PyTorch Model Architecture\n",
    "\n",
    "Explore the complete PyTorch backend with all model components:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model config: 2 layers, 1000 vocab size\n",
      "Using device: cpu\n",
      "RMSNorm input shape: torch.Size([2, 10, 64]), output shape: torch.Size([2, 10, 64])\n",
      "RoPE query shape: torch.Size([2, 10, 4, 16]) -> torch.Size([2, 10, 4, 16])\n"
     ]
    }
   ],
   "source": [
    "from gpt_oss.torch.model import (\n",
    "    ModelConfig, RMSNorm, RotaryEmbedding, AttentionBlock, \n",
    "    MLPBlock, TransformerBlock, Transformer\n",
    ")\n",
    "import torch\n",
    "\n",
    "# Model configuration (using smaller values for testing)\n",
    "config = ModelConfig(\n",
    "    num_hidden_layers=2,\n",
    "    vocab_size=1000,\n",
    "    hidden_size=64,\n",
    "    head_dim=16,\n",
    "    num_attention_heads=4,\n",
    "    num_key_value_heads=2,\n",
    "    initial_context_length=128\n",
    ")\n",
    "\n",
    "print(f\"Model config: {config.num_hidden_layers} layers, {config.vocab_size} vocab size\")\n",
    "\n",
    "# Device setup (force CPU for compatibility)\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# RMS Normalization\n",
    "rms_norm = RMSNorm(num_features=config.hidden_size, device=device)\n",
    "x = torch.randn(2, 10, config.hidden_size, device=device)\n",
    "normalized = rms_norm(x)\n",
    "print(f\"RMSNorm input shape: {x.shape}, output shape: {normalized.shape}\")\n",
    "\n",
    "# Rotary Embedding\n",
    "rope = RotaryEmbedding(\n",
    "    head_dim=config.head_dim,\n",
    "    base=config.rope_theta,\n",
    "    dtype=torch.float32,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "batch_size, seq_len = 2, 10\n",
    "query = torch.randn(batch_size, seq_len, config.num_attention_heads, config.head_dim, device=device)\n",
    "key = torch.randn(batch_size, seq_len, config.num_key_value_heads, config.head_dim, device=device)\n",
    "rotated_q, rotated_k = rope(query, key)\n",
    "print(f\"RoPE query shape: {query.shape} -> {rotated_q.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Advanced Tools and Complete Integration\n",
    "\n",
    "Custom tool creation and complete workflow example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Complete GPT-OSS Workflow Demo ===\n",
      "User input: Calculate 2 + 2 * 3\n",
      "Tokenized: 9 tokens\n",
      "Tool result: Result: 8\n",
      "Model: 36 layers, 201088 vocab\n",
      "Generated token: 659\n",
      "Workflow complete!\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "from gpt_oss.tokenizer import get_tokenizer\n",
    "from gpt_oss.torch.model import ModelConfig\n",
    "from gpt_oss.responses_api.utils import stub_infer_next_token\n",
    "from gpt_oss.tools.tool import Tool\n",
    "from openai_harmony import Message, Author, Role, TextContent\n",
    "from typing import AsyncIterator\n",
    "\n",
    "# Custom tool example\n",
    "class MathTool(Tool):\n",
    "    @property\n",
    "    def name(self) -> str:\n",
    "        return \"math_calculator\"\n",
    "    \n",
    "    @property\n",
    "    def instruction(self) -> str:\n",
    "        return \"A calculator tool that can evaluate mathematical expressions.\"\n",
    "    \n",
    "    async def _process(self, message: Message) -> AsyncIterator[Message]:\n",
    "        expression = message.content[0].text.strip()\n",
    "        \n",
    "        try:\n",
    "            # Safe evaluation of basic math expressions\n",
    "            allowed_names = {\n",
    "                'abs': abs, 'round': round, 'min': min, 'max': max, \n",
    "                'sum': sum, 'pow': pow, '__builtins__': {}\n",
    "            }\n",
    "            \n",
    "            result = eval(expression, allowed_names)\n",
    "            response_text = f\"Result: {result}\"\n",
    "        except Exception as e:\n",
    "            response_text = f\"Error: {str(e)}\"\n",
    "        \n",
    "        response = Message(\n",
    "            author=Author(role=Role.TOOL, name=self.name),\n",
    "            content=[TextContent(text=response_text)]\n",
    "        ).with_recipient(\"assistant\")\n",
    "        \n",
    "        yield response\n",
    "\n",
    "# Complete workflow demonstration\n",
    "async def complete_workflow_demo():\n",
    "    print(\"=== Complete GPT-OSS Workflow Demo ===\")\n",
    "    \n",
    "    # 1. Initialize components\n",
    "    tokenizer = get_tokenizer()\n",
    "    config = ModelConfig()\n",
    "    math_tool = MathTool()\n",
    "    \n",
    "    # 2. Process user input\n",
    "    user_input = \"Calculate 2 + 2 * 3\"\n",
    "    print(f\"User input: {user_input}\")\n",
    "    \n",
    "    # 3. Tokenization\n",
    "    tokens = tokenizer.encode(user_input)\n",
    "    print(f\"Tokenized: {len(tokens)} tokens\")\n",
    "    \n",
    "    # 4. Tool usage\n",
    "    tool_message = Message(\n",
    "        author=Author(role=Role.USER, name=\"user\"),\n",
    "        content=[TextContent(text=\"2 + 2 * 3\")]\n",
    "    )\n",
    "    \n",
    "    async for response in math_tool.process(tool_message):\n",
    "        print(f\"Tool result: {response.content[0].text}\")\n",
    "    \n",
    "    # 5. Model configuration info\n",
    "    print(f\"Model: {config.num_hidden_layers} layers, {config.vocab_size} vocab\")\n",
    "    \n",
    "    # 6. Stub generation\n",
    "    next_token = stub_infer_next_token(tokens[:5], temperature=0.7)\n",
    "    print(f\"Generated token: {next_token}\")\n",
    "    \n",
    "    print(\"Workflow complete!\")\n",
    "\n",
    "# For Jupyter notebooks, use await directly instead of asyncio.run()\n",
    "# This works because Jupyter already has an event loop running\n",
    "await complete_workflow_demo()\n",
    "\n",
    "# Note: If running in a regular Python script (not Jupyter), use:\n",
    "# asyncio.run(complete_workflow_demo())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Next Steps**\n",
    "To use GPT-OSS with actual models:\n",
    "1. Download or train model checkpoints\n",
    "2. Set up API keys for external tools (EXA_API_KEY for browser)\n",
    "3. Configure Docker for Python tool execution\n",
    "4. Scale to GPU/distributed setups for production use"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
