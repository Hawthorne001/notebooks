{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -qU google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "genai.configure(api_key=\"<KEY>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-09-30 23:57:04--  https://tridao.me/publications/flash3/flash3.pdf\n",
      "Resolving tridao.me (tridao.me)... 185.199.109.153, 185.199.110.153, 185.199.111.153, ...\n",
      "Connecting to tridao.me (tridao.me)|185.199.109.153|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 909666 (888K) [application/pdf]\n",
      "Saving to: ‘flash3.pdf’\n",
      "\n",
      "flash3.pdf          100%[===================>] 888.35K  --.-KB/s    in 0.04s   \n",
      "\n",
      "2024-09-30 23:57:04 (24.0 MB/s) - ‘flash3.pdf’ saved [909666/909666]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://tridao.me/publications/flash3/flash3.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = genai.GenerativeModel(\"gemini-1.5-flash\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "flash3 = genai.upload_file(\"./flash3.pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = model.generate_content([\"\"\"\n",
    "Summarize this PDF file with a focus on the methodology and key findings.\n",
    "Provide a concise summary of the arguments presented in this PDF, highlighting any controversies or debates.\n",
    "Extract the main points from this PDF and organize them by chapter or section.\n",
    "Give me a bullet-point summary of this PDF file, focusing on [specific topic or theme].\n",
    "\"\"\", flash3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The PDF file describes FlashAttention-3, a new algorithm for accelerating attention on GPUs. The algorithm leverages hardware features such as asynchrony and low precision to achieve significant speedups compared to prior methods.\n",
       "\n",
       "Here is a summary of the key arguments presented in the PDF:\n",
       "\n",
       "* **Attention is a bottleneck for large language models and long-context applications.** This is because the computational complexity of attention scales quadratically with the sequence length.\n",
       "* **FLASHATTENTION-2 achieved only 35% utilization on the H100 GPU.**  This highlights the need for new approaches to exploit the capabilities of modern GPUs.\n",
       "* **FLASHATTENTION-3 introduces three main techniques to speed up attention on Hopper GPUs:**\n",
       "    * **Exploiting asynchrony of the Tensor Cores and TMA.** This involves overlapping computation and data movement via warp-specialization and interleaving block-wise matmul and softmax operations.\n",
       "    * **Block quantization and incoherent processing.** These techniques leverage hardware support for FP8 low-precision.\n",
       "    * **Hiding softmax under asynchronous block-wise GEMMs.** This involves overlapping the comparatively low-throughput non-GEMM operations involved in softmax with the asynchronous WGMMA instructions for GEMM.\n",
       "\n",
       "The paper presents empirical results demonstrating the effectiveness of FLASHATTENTION-3. The authors report that:\n",
       "\n",
       "* **FP16 FLASHATTENTION-3 achieves a speedup of 1.5-2.0x over FLASHATTENTION-2.**\n",
       "* **FP8 FLASHATTENTION-3 achieves close to 1.2 PFLOPs/s.**\n",
       "* **FP8 FLASHATTENTION-3 achieves 2.6× lower numerical error than a baseline FP8 attention.**\n",
       "\n",
       "The authors also discuss several practical considerations related to implementing FLASHATTENTION-3, such as compiler reordering, register pressure, and layout transformations for FP8. Finally, they conclude that FLASHATTENTION-3 is a significant step forward in accelerating attention on GPUs, and that the techniques developed in this work will likely be applicable to other hardware accelerators and to other machine learning tasks. \n",
       "\n",
       "Here is a bullet-point summary of the PDF:\n",
       "\n",
       "* **Attention bottleneck:** Attention is a key bottleneck for scaling Transformer architectures to longer contexts.\n",
       "* **Prior work:**  FLASHATTENTION-2, while promising, did not fully exploit the capabilities of recent GPUs.\n",
       "* **FLASHATTENTION-3:** A new algorithm that significantly improves on FLASHATTENTION-2 by:\n",
       "    * **Leveraging asynchrony of the Tensor Cores and TMA:**  Overlapping computation and data movement.\n",
       "    * **Using FP8 low-precision:**  Achieving near-doubling of measured TFLOPs/s compared to FP16.\n",
       "    * **Hiding softmax under asynchronous block-wise GEMMs:**  Overlapping softmax computations.\n",
       "* **Empirical validation:** FLASHATTENTION-3 achieves significant speedups on H100 GPUs. FP8 implementation is also accurate.\n",
       "* **Future work:** Further optimizations and integration with existing software frameworks. \n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, Markdown\n",
    "display(Markdown(response.text))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
