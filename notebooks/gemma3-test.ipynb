{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hemanth/notebooks/blob/main/gemma3-test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#!/usr/bin/env python3\n",
        "\"\"\"\n",
        "Test script for Gemma 3 - Google's latest open model\n",
        "Based on the Jupyter notebook functionality\n",
        "\"\"\"\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import time\n",
        "import json\n",
        "\n",
        "def print_header(title):\n",
        "    \"\"\"Print a formatted header\"\"\"\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(f\" {title}\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "def test_basic_functionality():\n",
        "    \"\"\"Test basic Gemma 3 loading and inference\"\"\"\n",
        "    print_header(\"TESTING GEMMA 3 BASIC FUNCTIONALITY\")\n",
        "\n",
        "    # Model configuration\n",
        "    model_id = \"google/gemma-3-1b-it\"  # Using 1B instruct model\n",
        "\n",
        "    print(f\"üì¶ Loading model: {model_id}\")\n",
        "    print(\"‚è≥ This may take a few minutes for first-time download...\")\n",
        "\n",
        "    try:\n",
        "        # Load tokenizer\n",
        "        print(\"üîß Loading tokenizer...\")\n",
        "        tokenizer = AutoTokenizer.from_pretrained(\n",
        "            model_id,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        print(\"‚úÖ Tokenizer loaded successfully!\")\n",
        "\n",
        "        # Load model\n",
        "        print(\"üîß Loading model...\")\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_id,\n",
        "            torch_dtype=torch.bfloat16,\n",
        "            device_map=\"auto\",\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        print(\"‚úÖ Model loaded successfully!\")\n",
        "\n",
        "        # Print model info\n",
        "        total_params = sum(p.numel() for p in model.parameters())\n",
        "        print(f\"üìä Model parameters: {total_params:,}\")\n",
        "        print(f\"üñ•Ô∏è  Device: {next(model.parameters()).device}\")\n",
        "        print(f\"üî¢ Data type: {next(model.parameters()).dtype}\")\n",
        "\n",
        "        return model, tokenizer\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error loading model: {e}\")\n",
        "        return None, None\n",
        "\n",
        "def test_inference(model, tokenizer):\n",
        "    \"\"\"Test basic inference capabilities\"\"\"\n",
        "    print_header(\"TESTING INFERENCE CAPABILITIES\")\n",
        "\n",
        "    if model is None or tokenizer is None:\n",
        "        print(\"‚ùå Cannot test inference - model not loaded\")\n",
        "        return\n",
        "\n",
        "    # Test prompts\n",
        "    test_prompts = [\n",
        "        \"What is artificial intelligence?\",\n",
        "        \"Write a Python function to calculate fibonacci numbers:\",\n",
        "        \"Explain quantum computing in simple terms:\",\n",
        "        \"What are the benefits of renewable energy?\"\n",
        "    ]\n",
        "\n",
        "    for i, prompt in enumerate(test_prompts, 1):\n",
        "        print(f\"\\nüß™ Test {i}/4: {prompt}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        try:\n",
        "            # Tokenize input\n",
        "            inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "            if torch.backends.mps.is_available():\n",
        "                inputs = {k: v.to(\"mps\") for k, v in inputs.items()}\n",
        "\n",
        "            # Generate response\n",
        "            start_time = time.time()\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=150,\n",
        "                    temperature=0.7,\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=tokenizer.eos_token_id\n",
        "                )\n",
        "\n",
        "            end_time = time.time()\n",
        "\n",
        "            # Decode response\n",
        "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            generated_text = response[len(prompt):].strip()\n",
        "\n",
        "            print(f\"üí¨ Response: {generated_text}\")\n",
        "            print(f\"‚è±Ô∏è  Generation time: {end_time - start_time:.2f} seconds\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error during inference: {e}\")\n",
        "\n",
        "def test_multilingual_capabilities(model, tokenizer):\n",
        "    \"\"\"Test multilingual capabilities\"\"\"\n",
        "    print_header(\"TESTING MULTILINGUAL CAPABILITIES\")\n",
        "\n",
        "    if model is None or tokenizer is None:\n",
        "        print(\"‚ùå Cannot test multilingual - model not loaded\")\n",
        "        return\n",
        "\n",
        "    multilingual_prompts = [\n",
        "        (\"English\", \"Hello, how are you?\"),\n",
        "        (\"Spanish\", \"Hola, ¬øc√≥mo est√°s?\"),\n",
        "        (\"French\", \"Bonjour, comment allez-vous?\"),\n",
        "        (\"German\", \"Hallo, wie geht es dir?\"),\n",
        "        (\"Japanese\", \"„Åì„Çì„Å´„Å°„ÅØ„ÄÅÂÖÉÊ∞ó„Åß„Åô„ÅãÔºü\")\n",
        "    ]\n",
        "\n",
        "    for language, prompt in multilingual_prompts:\n",
        "        print(f\"\\nüåç Testing {language}: {prompt}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        try:\n",
        "            inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "            if torch.backends.mps.is_available():\n",
        "                inputs = {k: v.to(\"mps\") for k, v in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=100,\n",
        "                    temperature=0.7,\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=tokenizer.eos_token_id\n",
        "                )\n",
        "\n",
        "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            generated_text = response[len(prompt):].strip()\n",
        "\n",
        "            print(f\"üí¨ Response: {generated_text}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error with {language}: {e}\")\n",
        "\n",
        "def test_code_generation(model, tokenizer):\n",
        "    \"\"\"Test code generation capabilities\"\"\"\n",
        "    print_header(\"TESTING CODE GENERATION\")\n",
        "\n",
        "    if model is None or tokenizer is None:\n",
        "        print(\"‚ùå Cannot test code generation - model not loaded\")\n",
        "        return\n",
        "\n",
        "    code_prompts = [\n",
        "        \"Write a Python function to sort a list:\",\n",
        "        \"Create a JavaScript function to validate email:\",\n",
        "        \"Write a SQL query to find top 10 customers:\",\n",
        "        \"Create a React component for a button:\"\n",
        "    ]\n",
        "\n",
        "    for prompt in code_prompts:\n",
        "        print(f\"\\nüíª Code prompt: {prompt}\")\n",
        "        print(\"-\" * 50)\n",
        "\n",
        "        try:\n",
        "            inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "            if torch.backends.mps.is_available():\n",
        "                inputs = {k: v.to(\"mps\") for k, v in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=200,\n",
        "                    temperature=0.3,  # Lower temperature for more precise code\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=tokenizer.eos_token_id\n",
        "                )\n",
        "\n",
        "            response = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "            generated_code = response[len(prompt):].strip()\n",
        "\n",
        "            print(f\"üíª Generated code:\\n{generated_code}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error generating code: {e}\")\n",
        "\n",
        "def run_performance_benchmark(model, tokenizer):\n",
        "    \"\"\"Run performance benchmark\"\"\"\n",
        "    print_header(\"PERFORMANCE BENCHMARK\")\n",
        "\n",
        "    if model is None or tokenizer is None:\n",
        "        print(\"‚ùå Cannot run benchmark - model not loaded\")\n",
        "        return\n",
        "\n",
        "    benchmark_prompt = \"Write a detailed explanation of machine learning:\"\n",
        "\n",
        "    # Different token lengths\n",
        "    token_lengths = [50, 100, 200]\n",
        "\n",
        "    for max_tokens in token_lengths:\n",
        "        print(f\"\\n‚ö° Benchmark - {max_tokens} tokens:\")\n",
        "        print(\"-\" * 30)\n",
        "\n",
        "        try:\n",
        "            inputs = tokenizer(benchmark_prompt, return_tensors=\"pt\")\n",
        "            if torch.backends.mps.is_available():\n",
        "                inputs = {k: v.to(\"mps\") for k, v in inputs.items()}\n",
        "\n",
        "            start_time = time.time()\n",
        "            with torch.no_grad():\n",
        "                outputs = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=max_tokens,\n",
        "                    temperature=0.7,\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=tokenizer.eos_token_id\n",
        "                )\n",
        "            end_time = time.time()\n",
        "\n",
        "            generation_time = end_time - start_time\n",
        "            tokens_per_second = max_tokens / generation_time\n",
        "\n",
        "            print(f\"‚è±Ô∏è  Time: {generation_time:.2f} seconds\")\n",
        "            print(f\"üöÄ Speed: {tokens_per_second:.2f} tokens/second\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Benchmark error: {e}\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main test function\"\"\"\n",
        "    print_header(\"GEMMA 3 COMPREHENSIVE TEST SUITE\")\n",
        "    print(\"üî¨ Testing Google's Gemma 3 model capabilities\")\n",
        "    print(\"üìö Based on: https://blog.google/technology/developers/gemma-3/\")\n",
        "\n",
        "    # Load model\n",
        "    model, tokenizer = test_basic_functionality()\n",
        "\n",
        "    if model is not None and tokenizer is not None:\n",
        "        # Run all tests\n",
        "        test_inference(model, tokenizer)\n",
        "        test_multilingual_capabilities(model, tokenizer)\n",
        "        test_code_generation(model, tokenizer)\n",
        "        run_performance_benchmark(model, tokenizer)\n",
        "\n",
        "        print_header(\"TEST SUITE COMPLETED SUCCESSFULLY! üéâ\")\n",
        "        print(\"‚úÖ All tests completed\")\n",
        "        print(\"üìä Gemma 3 is working correctly on your system\")\n",
        "        print(\"üöÄ You can now use the model for your projects\")\n",
        "    else:\n",
        "        print_header(\"TEST SUITE FAILED ‚ùå\")\n",
        "        print(\"‚ùå Could not load Gemma 3 model\")\n",
        "        print(\"üí° Check your internet connection and try again\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BlipSyGXVx0",
        "outputId": "eb3c3d3d-dbde-445e-e20e-075b27d9c108"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            " GEMMA 3 COMPREHENSIVE TEST SUITE\n",
            "============================================================\n",
            "üî¨ Testing Google's Gemma 3 model capabilities\n",
            "üìö Based on: https://blog.google/technology/developers/gemma-3/\n",
            "\n",
            "============================================================\n",
            " TESTING GEMMA 3 BASIC FUNCTIONALITY\n",
            "============================================================\n",
            "üì¶ Loading model: google/gemma-3-1b-it\n",
            "‚è≥ This may take a few minutes for first-time download...\n",
            "üîß Loading tokenizer...\n",
            "‚úÖ Tokenizer loaded successfully!\n",
            "üîß Loading model...\n",
            "‚úÖ Model loaded successfully!\n",
            "üìä Model parameters: 999,885,952\n",
            "üñ•Ô∏è  Device: cpu\n",
            "üî¢ Data type: torch.bfloat16\n",
            "\n",
            "============================================================\n",
            " TESTING INFERENCE CAPABILITIES\n",
            "============================================================\n",
            "\n",
            "üß™ Test 1/4: What is artificial intelligence?\n",
            "--------------------------------------------------\n",
            "üí¨ Response: Artificial intelligence (AI) refers to the simulation of human intelligence processes by computer systems. It‚Äôs not about creating robots that think exactly like us, but rather about developing computer systems that can perform tasks that typically require human intelligence, such as learning, problem-solving, and decision-making.\n",
            "\n",
            "Here's a breakdown of key aspects:\n",
            "\n",
            "* **Machine Learning (ML):** A subset of AI where systems learn from data without explicit programming.\n",
            "* **Deep Learning:** A more advanced form of machine learning using artificial neural networks with multiple layers.\n",
            "* **Natural Language Processing (NLP):**  Focuses on enabling computers to understand, interpret, and generate human language.\n",
            "* **Computer Vision:** Allows computers to \"see\n",
            "‚è±Ô∏è  Generation time: 81.96 seconds\n",
            "\n",
            "üß™ Test 2/4: Write a Python function to calculate fibonacci numbers:\n",
            "--------------------------------------------------\n",
            "üí¨ Response: ```python\n",
            "def fibonacci_sequence(n):\n",
            "  \"\"\"\n",
            "  Calculates the Fibonacci sequence up to n terms.\n",
            "\n",
            "  Args:\n",
            "    n: The number of terms in the sequence.\n",
            "\n",
            "  Returns:\n",
            "    A list containing the Fibonacci sequence up to n terms.\n",
            "  \"\"\"\n",
            "  if n <= 0:\n",
            "    return []\n",
            "  elif n == 1:\n",
            "    return [0]\n",
            "  else:\n",
            "    list_fib = [0, 1]\n",
            "    while len(list_fib) < n:\n",
            "      next_fib = list_fib[-1] + list_fib[-2]\n",
            "      list_fib.append(next_fib)\n",
            "‚è±Ô∏è  Generation time: 71.22 seconds\n",
            "\n",
            "üß™ Test 3/4: Explain quantum computing in simple terms:\n",
            "--------------------------------------------------\n",
            "üí¨ Response: Okay, so let's talk about quantum computing. It‚Äôs a really different way of processing information compared to the computers we use every day.  Here's the basic idea:\n",
            "\n",
            "* **Classical computers:**  Think of them like light switches. They can be either on (1) or off (0).  They store information as bits, which are either a 0 or a 1.  Everything a classical computer does ‚Äì from writing emails to playing games ‚Äì is based on manipulating these bits.\n",
            "\n",
            "* **Quantum computers:**  Now, imagine a dimmer switch instead of a light switch.  A quantum computer uses \"qubits.\"  A qubit isn‚Äôt just 0 or 1; it can be\n",
            "‚è±Ô∏è  Generation time: 68.20 seconds\n",
            "\n",
            "üß™ Test 4/4: What are the benefits of renewable energy?\n",
            "--------------------------------------------------\n",
            "üí¨ Response: Renewable energy sources, such as solar, wind, hydro, and geothermal, offer a multitude of benefits compared to traditional fossil fuels. Here's a breakdown:\n",
            "\n",
            "*   **Environmental Benefits:**\n",
            "    *   **Reduced Greenhouse Gas Emissions:** Renewable energy sources produce little to no greenhouse gases during operation, mitigating climate change.\n",
            "    *   **Reduced Air Pollution:** They don't release harmful pollutants like sulfur dioxide and nitrogen oxides, improving air quality and public health.\n",
            "    *   **Water Conservation:** Many renewable technologies, like solar and wind, require significantly less water than fossil fuel power plants.\n",
            "\n",
            "*   **Economic Benefits:**\n",
            "    *   **Job Creation:** The renewable energy sector is growing rapidly, creating new jobs\n",
            "‚è±Ô∏è  Generation time: 64.36 seconds\n",
            "\n",
            "============================================================\n",
            " TESTING MULTILINGUAL CAPABILITIES\n",
            "============================================================\n",
            "\n",
            "üåç Testing English: Hello, how are you?\n",
            "--------------------------------------------------\n",
            "üí¨ Response: I'm working on a project that involves creating a system for automatically generating summaries of long documents.  I'm currently using a simple approach ‚Äì just taking a chunk of the text and returning a short summary. However, I'm hitting a wall.  I want to move beyond just a simple sentence or two and produce more nuanced, context-aware summaries.\n",
            "\n",
            "Here's what I've tried so far:\n",
            "\n",
            "1.  **Keyword Extraction:** I use a library like\n",
            "\n",
            "üåç Testing Spanish: Hola, ¬øc√≥mo est√°s?\n",
            "--------------------------------------------------\n",
            "üí¨ Response: Soy un modelo de lenguaje grande, entrenado por Google.\n",
            "\n",
            "¬øQu√© necesitas de m√≠ hoy?\n",
            "\n",
            "üåç Testing French: Bonjour, comment allez-vous?\n",
            "--------------------------------------------------\n",
            "üí¨ Response: Je vous propose de faire une petite discussion sur les possibilit√©s d'un projet de jardinage.\n",
            "\n",
            "*   **Quel est votre niveau d'exp√©rience en jardinage ?** (D√©butant, interm√©diaire, expert)\n",
            "*   **Quels sont vos objectifs ?** (Cr√©er un jardinage esth√©tique, produire des l√©gumes, cultiver des fleurs, etc.)\n",
            "*   **Quel est votre espace disponible ?** (Petit balcon, jardin, terrain, etc.)\n",
            "*\n",
            "\n",
            "üåç Testing German: Hallo, wie geht es dir?\n",
            "--------------------------------------------------\n",
            "üí¨ Response: Ich habe eine Frage zu einem Thema, das ich gerade in der Schule lernen.\n",
            "\n",
            "Ich habe eine Frage:\n",
            "**Wenn ein Mensch einen Tag lang ein Tier sein kann, wie w√ºrde sich das f√ºr ihn/sie?**\n",
            "\n",
            "Ich bin mir nicht sicher, ob ich das √ºberhaupt richtig verstehst.\n",
            "\n",
            "Kannst du mir sagen, wie sich das f√ºr ihn/sie ver√§ndern k√∂nnte?\n",
            "\n",
            "Lass mich wissen, wenn du Hilfe brauchst!\n",
            "\n",
            "üåç Testing Japanese: „Åì„Çì„Å´„Å°„ÅØ„ÄÅÂÖÉÊ∞ó„Åß„Åô„ÅãÔºü\n",
            "--------------------------------------------------\n",
            "üí¨ Response: ‰ªäÊó•„ÅØ„ÅÑ„ÅÑÂ§©Ê∞ó„Åß„Åô„Å≠ÔºÅ\n",
            "Êó•Â∑Æ„Åó„ÅåÊ∞óÊåÅ„Å°„ÅÑ„ÅÑ„Åß„Åô„ÄÇ\n",
            "\n",
            "‰Ωï„ÅãÈù¢ÁôΩ„ÅÑ„Åì„Å®„ÇÑ„ÄÅÊ•Ω„Åó„ÅÑ„Åì„Å®„ÄÅ\n",
            "‰ªäÊó•„ÅÇ„Å£„Åü„Åì„Å®„Å™„Å©„ÄÅ\n",
            "Êïô„Åà„Å¶„ÅÑ„Åü„Å†„Åë„Åæ„Åõ„Çì„ÅãÔºü\n",
            "\n",
            "Ë©±„ÇíËÅû„Åè„Åì„Å®„Åó„Åã„Åß„Åç„Åæ„Åõ„Çì„Åå„ÄÅ\n",
            "„ÅÑ„Å§„Åß„ÇÇ„ÅÇ„Å™„Åü„ÅÆ„Åì„Å®„ÇíËÅû„ÅÑ„Å¶„Åè„Å†„Åï„ÅÑ„Å≠„ÄÇ\n",
            "```\n",
            "\n",
            "---\n",
            "\n",
            "„Åì„ÅÆ„É°„ÉÉ„Çª„Éº„Ç∏„ÅØ„ÄÅÁõ∏Êâã„Å´„ÄåÂÖÉÊ∞ó„Åß„Åô„ÅãÔºü„Äç„Å®Â∞ã„Å≠„ÄÅÁõ∏Êâã„ÅÆÁä∂Ê≥Å„ÇÑË©±„ÇíËÅû„Åè„Åì„Å®„ÅÆÊ∞óÊåÅ„Å°„Çí‰ºù„Åà„ÄÅÁõ∏Êâã„ÅåË©±„Åó„Åü„ÅÑÊôÇ„Å´Ë©±„Åõ„Çã„Çà„ÅÜ„Å´‰øÉ„Åô„ÄÅ„Å®„ÅÑ„ÅÜÁõÆÁöÑ„ÅßÊõ∏„Åã„Çå„Å¶„ÅÑ„Åæ„Åô\n",
            "\n",
            "============================================================\n",
            " TESTING CODE GENERATION\n",
            "============================================================\n",
            "\n",
            "üíª Code prompt: Write a Python function to sort a list:\n",
            "--------------------------------------------------\n",
            "üíª Generated code:\n",
            "```python\n",
            "def sort_list(data):\n",
            "  \"\"\"Sorts a list in ascending order.\n",
            "\n",
            "  Args:\n",
            "    data: A list of numbers.\n",
            "\n",
            "  Returns:\n",
            "    A new list containing the sorted elements.\n",
            "  \"\"\"\n",
            "  new_list = sorted(data)\n",
            "  return new_list\n",
            "\n",
            "# Example usage:\n",
            "my_list = [5, 2, 8, 1, 9]\n",
            "sorted_list = sort_list(my_list)\n",
            "print(sorted_list)  # Output: [1, 2, 5, 8, 9]\n",
            "```\n",
            "\n",
            "**Explanation:**\n",
            "\n",
            "1.  **`def sort_list(data):`**: This line defines a function named `sort_list` that takes one argument: `data`, which is expected to be a list.\n",
            "2.  **`new_list = sorted(data)`**: This line uses the built\n",
            "\n",
            "üíª Code prompt: Create a JavaScript function to validate email:\n",
            "--------------------------------------------------\n",
            "üíª Generated code:\n",
            "```javascript\n",
            "function validateEmail(email) {\n",
            "  // Input validation\n",
            "  if (typeof email !== 'string') {\n",
            "    return \"Invalid input: Email must be a string.\";\n",
            "  }\n",
            "\n",
            "  // Check if the email is empty\n",
            "  if (!email) {\n",
            "    return \"Invalid input: Email cannot be empty.\";\n",
            "  }\n",
            "\n",
            "  // Regular expression for email validation\n",
            "  const emailRegex = /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/;\n",
            "  if (!emailRegex.test(email)) {\n",
            "    return \"Invalid email format.\";\n",
            "  }\n",
            "\n",
            "  return \"Valid email.\";\n",
            "}\n",
            "\n",
            "// Example usage:\n",
            "console.log(validateEmail(\"test@example.com\")); // Output: Valid email.\n",
            "console.log(validateEmail(\"test.example.com\")); // Output: Invalid email format.\n",
            "console.log(validateEmail(\"\")); // Output: Invalid input: Email\n",
            "\n",
            "üíª Code prompt: Write a SQL query to find top 10 customers:\n",
            "--------------------------------------------------\n",
            "üíª Generated code:\n",
            "```sql\n",
            "SELECT\n",
            "    customer_id,\n",
            "    customer_name,\n",
            "    SUM(order_total) AS total_spent\n",
            "FROM\n",
            "    Orders\n",
            "GROUP BY\n",
            "    customer_id,\n",
            "    customer_name\n",
            "ORDER BY\n",
            "    total_spent DESC\n",
            "LIMIT 10;\n",
            "```\n",
            "\n",
            "**Explanation:**\n",
            "\n",
            "1.  **`SELECT customer_id, customer_name, SUM(order_total) AS total_spent`**: This selects the customer ID, customer name, and the sum of order totals for each customer.  `SUM(order_total)` calculates the total amount spent by each customer.  `AS total_spent` gives an alias to the calculated sum, making it easier to refer to.\n",
            "2.  **`FROM Orders`**: This specifies that we are retrieving data from the `Orders` table.  (Assume the table is named `Orders` and contains columns like `customer_id`, `customer_\n",
            "\n",
            "üíª Code prompt: Create a React component for a button:\n",
            "--------------------------------------------------\n",
            "üíª Generated code:\n",
            "```javascript\n",
            "import React from 'react';\n",
            "\n",
            "function Button({ text, onClick, disabled }) {\n",
            "  return (\n",
            "    <button onClick={onClick} disabled={disabled}>\n",
            "      {text}\n",
            "    </button>\n",
            "  );\n",
            "}\n",
            "\n",
            "export default Button;\n",
            "```\n",
            "\n",
            "**Explanation:**\n",
            "\n",
            "*   **`import React from 'react';`**: Imports the React library.\n",
            "*   **`function Button({ text, onClick, disabled }) { ... }`**: Defines a functional React component named `Button`.  It accepts three props:\n",
            "    *   `text`: The text to display on the button.\n",
            "    *   `onClick`: A function to be called when the button is clicked.\n",
            "    *   `disabled`: A boolean indicating whether the button is disabled.\n",
            "*   **`return ( ... )`**:  Returns the JSX (JavaScript XML) that describes the component's UI.\n",
            "*   **`<button onClick={onClick\n",
            "\n",
            "============================================================\n",
            " PERFORMANCE BENCHMARK\n",
            "============================================================\n",
            "\n",
            "‚ö° Benchmark - 50 tokens:\n",
            "------------------------------\n",
            "‚è±Ô∏è  Time: 23.33 seconds\n",
            "üöÄ Speed: 2.14 tokens/second\n",
            "\n",
            "‚ö° Benchmark - 100 tokens:\n",
            "------------------------------\n",
            "‚è±Ô∏è  Time: 42.83 seconds\n",
            "üöÄ Speed: 2.33 tokens/second\n",
            "\n",
            "‚ö° Benchmark - 200 tokens:\n",
            "------------------------------\n",
            "‚è±Ô∏è  Time: 85.40 seconds\n",
            "üöÄ Speed: 2.34 tokens/second\n",
            "\n",
            "============================================================\n",
            " TEST SUITE COMPLETED SUCCESSFULLY! üéâ\n",
            "============================================================\n",
            "‚úÖ All tests completed\n",
            "üìä Gemma 3 is working correctly on your system\n",
            "üöÄ You can now use the model for your projects\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
